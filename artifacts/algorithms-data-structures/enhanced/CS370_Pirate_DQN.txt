import random
from collections import deque
import numpy as np

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam


# -----------------------------
# Environment: Pirate Treasure GridWorld (Enhanced)
# -----------------------------
class PirateTreasureEnv:
    """
    Enhanced grid world environment for the pirate NPC.

    Enhancements vs. original version:
    - Reward shaping using distance to the treasure.
    - Penalty for revisiting previously visited cells to discourage loops.
    - Richer state representation that includes normalized distance
      to the treasure to help the network learn a better value function.
    """

    def __init__(self, grid_size=5, max_steps=50):
        self.grid_size = grid_size
        self.max_steps = max_steps

        # Define treasure and obstacles
        self.treasure_pos = (grid_size - 1, grid_size - 1)  # bottom-right corner
        self.obstacles = {(1, 1), (2, 2), (3, 1)}  # example obstacle positions

        # Action space: 0=up, 1=down, 2=left, 3=right
        self.action_space_n = 4

        # State: (pirate_row, pirate_col)
        self.state = None
        self.steps_taken = 0
        self.visited = set()

    def reset(self):
        """Reset the environment for a new episode."""
        self.state = (0, 0)  # can randomize if desired
        self.steps_taken = 0
        self.visited = {self.state}
        return self._get_state_vector()

    def step(self, action):
        """
        Take a step in the environment.

        Returns:
            next_state (np.array): next state vector
            reward (float): reward for this step
            done (bool): whether the episode is finished
            info (dict): extra information
        """
        self.steps_taken += 1
        row, col = self.state

        # Keep track of previous distance for reward shaping
        prev_dist = self._manhattan_distance(self.state, self.treasure_pos)

        # Apply action
        if action == 0:      # up
            row -= 1
        elif action == 1:    # down
            row += 1
        elif action == 2:    # left
            col -= 1
        elif action == 3:    # right
            col += 1

        # Stay within bounds
        row = max(0, min(self.grid_size - 1, row))
        col = max(0, min(self.grid_size - 1, col))

        # Check for obstacle: if obstacle, revert to previous state
        if (row, col) in self.obstacles:
            row, col = self.state

        self.state = (row, col)

        # Base step cost to encourage shorter paths
        reward = -1.0
        done = False

        # Distance-based shaping: reward moving closer to treasure,
        # penalize moving farther away.
        new_dist = self._manhattan_distance(self.state, self.treasure_pos)
        if new_dist < prev_dist:
            reward += 0.5   # small bonus for progress
        elif new_dist > prev_dist:
            reward -= 0.5   # penalty for moving away

        # Penalize revisiting cells to avoid loops
        # (if we've been in this cell before)
        # Note: add after computing reward so first visit is "free"
        if self.state in self.visited:
            reward -= 0.5
        self.visited.add(self.state)

        # Terminal conditions
        if self.state == self.treasure_pos:
            reward = 50.0  # big positive reward for finding treasure
            done = True
        elif self.steps_taken >= self.max_steps:
            reward -= 10.0  # additional penalty for taking too long
            done = True

        info = {"distance_to_treasure": new_dist}
        return self._get_state_vector(), reward, done, info

    def _get_state_vector(self):
        """
        Convert (row, col) into a richer numeric vector.

        State vector:
        - normalized row position
        - normalized column position
        - normalized Manhattan distance to the treasure
        """
        row, col = self.state
        max_index = self.grid_size - 1

        norm_row = row / max_index
        norm_col = col / max_index
        max_dist = (self.grid_size - 1) * 2  # max Manhattan distance
        dist = self._manhattan_distance(self.state, self.treasure_pos)
        norm_dist = dist / max_dist

        return np.array([norm_row, norm_col, norm_dist], dtype=np.float32)

    @staticmethod
    def _manhattan_distance(a, b):
        (r1, c1), (r2, c2) = a, b
        return abs(r1 - r2) + abs(c1 - c2)


# -----------------------------
# Deep Q-Network (DQN) Agent with Target Network
# -----------------------------
class DQNAgent:
    """
    Deep Q-Learning agent for the pirate NPC.

    Enhancements vs. original version:
    - Uses a separate target network for more stable learning.
    - Adds a method to update the target network on a fixed schedule.
    - Adds a helper method for greedy action selection during evaluation.
    """

    def __init__(
        self,
        state_size,
        action_size,
        learning_rate=0.001,
        gamma=0.99,
        epsilon=1.0,
        epsilon_min=0.01,
        epsilon_decay=0.995,
        batch_size=64,
        memory_size=2000,
    ):
        self.state_size = state_size
        self.action_size = action_size

        self.gamma = gamma  # discount factor
        self.epsilon = epsilon  # exploration rate
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay

        self.learning_rate = learning_rate
        self.batch_size = batch_size

        # Replay memory
        self.memory = deque(maxlen=memory_size)

        # Main Q-network and target network
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_network()  # initialize with same weights

    def _build_model(self):
        """
        Build a simple feedforward neural network for Q-value approximation.
        """
        model = Sequential()
        model.add(Dense(64, input_dim=self.state_size, activation="relu"))
        model.add(Dense(64, activation="relu"))
        model.add(Dense(self.action_size, activation="linear"))

        model.compile(loss="mse", optimizer=Adam(learning_rate=self.learning_rate))
        return model

    def update_target_network(self):
        """Copy weights from the main network to the target network."""
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        """
        Store experience tuple in replay memory.
        """
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        """
        Choose an action using epsilon-greedy policy.
        """
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        q_values = self.model.predict(state[np.newaxis, :], verbose=0)
        return int(np.argmax(q_values[0]))

    def act_greedy(self, state):
        """
        Choose the best action without exploration (for evaluation).
        """
        q_values = self.model.predict(state[np.newaxis, :], verbose=0)
        return int(np.argmax(q_values[0]))

    def replay(self):
        """
        Sample a batch from memory and train the Q-network.
        """
        if len(self.memory) < self.batch_size:
            return  # not enough samples yet

        minibatch = random.sample(self.memory, self.batch_size)

        states = np.zeros((self.batch_size, self.state_size), dtype=np.float32)
        targets = np.zeros((self.batch_size, self.action_size), dtype=np.float32)

        for i, (state, action, reward, next_state, done) in enumerate(minibatch):
            # Current Q-values from the main network
            target = self.model.predict(state[np.newaxis, :], verbose=0)[0]

            if done:
                target[action] = reward
            else:
                # Use target network for next-state value estimation
                next_q_values = self.target_model.predict(
                    next_state[np.newaxis, :], verbose=0
                )[0]
                target[action] = reward + self.gamma * np.max(next_q_values)

            states[i] = state
            targets[i] = target

        # Train the main network on this batch
        self.model.fit(states, targets, epochs=1, verbose=0)

        # Decay exploration rate
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay


# -----------------------------
# Training and Evaluation
# -----------------------------
def train_pirate_agent(
    episodes=500,
    grid_size=5,
    max_steps=50,
    render_interval=50,
    target_update_interval=10,
):
    """
    Train the pirate DQN agent in the treasure environment.

    Args:
        episodes (int): number of training episodes
        grid_size (int): size of the grid world
        max_steps (int): max steps per episode
        render_interval (int): how often to print progress to console
        target_update_interval (int): how often (in episodes) to update the target network
    """
    env = PirateTreasureEnv(grid_size=grid_size, max_steps=max_steps)

    # State now has 3 features: [row, col, distance]
    state_size = 3
    action_size = env.action_space_n

    agent = DQNAgent(
        state_size=state_size,
        action_size=action_size,
        learning_rate=0.001,
        gamma=0.99,
        epsilon=1.0,
        epsilon_min=0.01,
        epsilon_decay=0.995,
        batch_size=64,
        memory_size=5000,
    )

    reward_history = []

    for episode in range(1, episodes + 1):
        state = env.reset()
        total_reward = 0

        for step in range(max_steps):
            # Agent selects action
            action = agent.act(state)

            # Environment responds
            next_state, reward, done, _ = env.step(action)

            # Store experience and learn
            agent.remember(state, action, reward, next_state, done)
            agent.replay()

            state = next_state
            total_reward += reward

            if done:
                break

        reward_history.append(total_reward)

        # Periodically update the target network
        if episode % target_update_interval == 0:
            agent.update_target_network()

        # Logging
        if episode % render_interval == 0:
            avg_reward = np.mean(reward_history[-render_interval:])
            print(
                f"Episode {episode}/{episodes} - "
                f"Total Reward: {total_reward:.2f}, "
                f"Average Reward (last {render_interval}): {avg_reward:.2f}, "
                f"Epsilon: {agent.epsilon:.3f}"
            )

    print("Training finished.")
    return agent, reward_history


def evaluate_agent(agent, grid_size=5, max_steps=50, episodes=50):
    """
    Evaluate a trained agent using a greedy policy (no exploration).

    Returns:
        success_rate (float): fraction of episodes where the treasure was reached
        average_steps (float): average number of steps taken per episode
    """
    env = PirateTreasureEnv(grid_size=grid_size, max_steps=max_steps)
    successes = 0
    steps_list = []

    for _ in range(episodes):
        state = env.reset()
        for step in range(max_steps):
            action = agent.act_greedy(state)
            next_state, _, done, _ = env.step(action)
            state = next_state
            if done:
                if env.state == env.treasure_pos:
                    successes += 1
                steps_list.append(step + 1)
                break

    success_rate = successes / episodes
    average_steps = np.mean(steps_list) if steps_list else max_steps
    print(
        f"Evaluation over {episodes} episodes - "
        f"Success rate: {success_rate:.2%}, "
        f"Average steps: {average_steps:.2f}"
    )
    return success_rate, average_steps


if __name__ == "__main__":
    # Example training run for the enhanced DQN pirate agent.
    trained_agent, rewards = train_pirate_agent(
        episodes=300,
        grid_size=5,
        max_steps=40,
        render_interval=25,
        target_update_interval=10,
    )

    # Optional: evaluate the trained agent without exploration
    evaluate_agent(trained_agent, grid_size=5, max_steps=40, episodes=50)

    # Optional: save the trained model for reuse in your ePortfolio
    # trained_agent.model.save("pirate_dqn_enhanced.h5")
